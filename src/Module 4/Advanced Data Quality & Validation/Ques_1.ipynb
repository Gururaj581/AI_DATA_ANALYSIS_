{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 3.23\n",
      "Drifted data MSE: 3.65\n"
     ]
    }
   ],
   "source": [
    "# Data Drift Impact on Model\n",
    "# Question: Use a simple linear regression model to demonstrate how data drift affects model predictions.\n",
    "\n",
    "# 1. Train a model on the original data:\n",
    "# 2. Evaluate on the drifted data:\n",
    "# 3. Compare errors:\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Original data (training)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(100, 1) * 10\n",
    "y_train = 2 * X_train.squeeze() + 1 + np.random.randn(100) * 2\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Drifted data (shifted feature distribution)\n",
    "X_drifted = np.random.rand(100, 1) * 10 + 5  # Shifted distribution\n",
    "y_drifted = 2 * X_drifted.squeeze() + 1 + np.random.randn(100) * 2\n",
    "\n",
    "# Evaluate on original and drifted data\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_drifted = model.predict(X_drifted)\n",
    "\n",
    "# 3. Compare errors\n",
    "train_error = mean_squared_error(y_train, y_pred_train)\n",
    "drifted_error = mean_squared_error(y_drifted, y_pred_drifted)\n",
    "\n",
    "print(f\"Training MSE: {train_error:.2f}\")\n",
    "print(f\"Drifted data MSE: {drifted_error:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mean: 0.02, Drifted mean: 1.09\n",
      "Original std: 0.98, Drifted std: 1.20\n",
      "KS-test p-value: 0.0000\n",
      "Warning: Significant data drift detected!\n",
      "- Mean changed by 1.07\n",
      "- Distribution shape changed (KS-test)\n"
     ]
    }
   ],
   "source": [
    "# Monitoring Data Distribution Changes\n",
    "# Question: Use Python to monitor distribution changes in features to detect potential data drift.\n",
    "\n",
    "# 1. Calculate feature statistics (mean and standard deviation) for both original and drifted data:\n",
    "# 2. Compare statistics:\n",
    "# 3. Set thresholds to detect significant drift:\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# 1. Original and drifted data\n",
    "np.random.seed(42)\n",
    "original_data = np.random.normal(0, 1, 1000)\n",
    "drifted_data = np.random.normal(1, 1.2, 1000)  # Mean and std deviation changed\n",
    "\n",
    "# 2. Calculate statistics\n",
    "original_mean, original_std = np.mean(original_data), np.std(original_data)\n",
    "drifted_mean, drifted_std = np.mean(drifted_data), np.std(drifted_data)\n",
    "\n",
    "# 3. Compare distributions and detect drift\n",
    "ks_stat, p_value = ks_2samp(original_data, drifted_data)\n",
    "\n",
    "# Set thresholds\n",
    "MEAN_THRESHOLD = 0.5\n",
    "STD_THRESHOLD = 0.3\n",
    "KS_P_THRESHOLD = 0.05\n",
    "\n",
    "print(f\"Original mean: {original_mean:.2f}, Drifted mean: {drifted_mean:.2f}\")\n",
    "print(f\"Original std: {original_std:.2f}, Drifted std: {drifted_std:.2f}\")\n",
    "print(f\"KS-test p-value: {p_value:.4f}\")\n",
    "\n",
    "# Drift detection\n",
    "mean_drift = abs(original_mean - drifted_mean) > MEAN_THRESHOLD\n",
    "std_drift = abs(original_std - drifted_std) > STD_THRESHOLD\n",
    "ks_drift = p_value < KS_P_THRESHOLD\n",
    "\n",
    "if mean_drift or std_drift or ks_drift:\n",
    "    print(\"Warning: Significant data drift detected!\")\n",
    "    if mean_drift:\n",
    "        print(f\"- Mean changed by {abs(original_mean - drifted_mean):.2f}\")\n",
    "    if std_drift:\n",
    "        print(f\"- Std deviation changed by {abs(original_std - drifted_std):.2f}\")\n",
    "    if ks_drift:\n",
    "        print(\"- Distribution shape changed (KS-test)\")\n",
    "else:\n",
    "    print(\"No significant drift detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Report:\n",
      "Total rows: 5\n",
      "Missing values: 1\n",
      "Duplicate rows: 0\n",
      "Negative values in numeric columns: 1\n",
      "Zero values in numeric columns: 1\n",
      "\n",
      "Data Types:\n",
      "id            int64\n",
      "value       float64\n",
      "category     object\n",
      "dtype: object\n",
      "\n",
      "Data validation status: Failed\n"
     ]
    }
   ],
   "source": [
    "# Automating Data Quality Checks with Python\n",
    "# Question: Automate a basic data validation process using Python to ensure the dataset's\n",
    "# structural integrity.\n",
    "\n",
    "# 1. Define validation checks:\n",
    "# 2. Apply validation:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def validate_data(df):\n",
    "    # 1. Define validation checks\n",
    "    checks = {\n",
    "        'missing_values': df.isnull().sum().sum(),\n",
    "        'duplicate_rows': df.duplicated().sum(),\n",
    "        'negative_values': (df.select_dtypes(include=['number']) < 0).sum().sum(),\n",
    "        'zero_values': (df.select_dtypes(include=['number']) == 0).sum().sum(),\n",
    "        'data_types': df.dtypes\n",
    "    }\n",
    "    \n",
    "    # 2. Apply validation and report results\n",
    "    print(\"Data Quality Report:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Missing values: {checks['missing_values']}\")\n",
    "    print(f\"Duplicate rows: {checks['duplicate_rows']}\")\n",
    "    print(f\"Negative values in numeric columns: {checks['negative_values']}\")\n",
    "    print(f\"Zero values in numeric columns: {checks['zero_values']}\")\n",
    "    print(\"\\nData Types:\")\n",
    "    print(checks['data_types'])\n",
    "    \n",
    "    # Return validation status\n",
    "    if checks['missing_values'] > 0 or checks['duplicate_rows'] > 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    'id': [1, 2, 3, 4, 4],\n",
    "    'value': [10, -5, 0, 15, None],\n",
    "    'category': ['A', 'B', 'A', 'C', 'C']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "is_valid = validate_data(df)\n",
    "\n",
    "print(f\"\\nData validation status: {'Passed' if is_valid else 'Failed'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigNotFoundError",
     "evalue": "Error: No great_expectations directory was found here!\n    - Please check that you are in the correct directory or have specified the correct directory.\n    - If you have never run Great Expectations in this project, please run `great_expectations init` to get started.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Introducing Great Expectations for Data Validation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Question: Use Great Expectations to set up data validation checks for a dataset.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Install Great Expectations:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2. Create a new expectations suite:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 3. Load data and generate expectations:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m context\u001b[38;5;241m=\u001b[39m\u001b[43mge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m suite\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mcreate_expectation_suite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m df\u001b[38;5;241m=\u001b[39mge\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context.py:2249\u001b[0m, in \u001b[0;36mDataContext.__init__\u001b[0;34m(self, context_root_dir, runtime_environment)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, context_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, runtime_environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2246\u001b[0m \n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;66;03m# Determine the \"context root directory\" - this is the parent of \"great_expectations\" dir\u001b[39;00m\n\u001b[1;32m   2248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_root_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2249\u001b[0m         context_root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_context_root_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m     context_root_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(context_root_dir))\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_root_directory \u001b[38;5;241m=\u001b[39m context_root_directory\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/great_expectations/data_context/data_context.py:2368\u001b[0m, in \u001b[0;36mDataContext.find_context_root_dir\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   2365\u001b[0m         result \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(yml_path)\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ge_exceptions\u001b[38;5;241m.\u001b[39mConfigNotFoundError()\n\u001b[1;32m   2370\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing project config: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(yml_path))\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mConfigNotFoundError\u001b[0m: Error: No great_expectations directory was found here!\n    - Please check that you are in the correct directory or have specified the correct directory.\n    - If you have never run Great Expectations in this project, please run `great_expectations init` to get started.\n"
     ]
    }
   ],
   "source": [
    "# Introducing Great Expectations for Data Validation\n",
    "# Question: Use Great Expectations to set up data validation checks for a dataset.\n",
    "\n",
    "# 1. Install Great Expectations:\n",
    "# 2. Create a new expectations suite:\n",
    "# 3. Load data and generate expectations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4049000360.py, line 109)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 109\u001b[0;36m\u001b[0m\n\u001b[0;31m    if result['null_count'] > :\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Automating Constraint Checks with Python\n",
    "# Question: Automate primary key and foreign key constraint checks using Python to ensure dataset compliance.\n",
    "# 1. Assuming datasets exist with primary and foreign key relationships in pandas dataframes employees_df and departments_df :\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "class ConstraintValidator:\n",
    "    def __init__(self, employees_df: pd.DataFrame, departments_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize validator with employee and department DataFrames\n",
    "        \n",
    "        Args:\n",
    "            employees_df: DataFrame containing employee records\n",
    "            departments_df: DataFrame containing department records\n",
    "        \"\"\"\n",
    "        self.employees = employees_df\n",
    "        self.departments = departments_df\n",
    "        \n",
    "        # Configurable column names (can be modified as needed)\n",
    "        self.emp_pk = 'emp_id'\n",
    "        self.emp_fk = 'dept_id'\n",
    "        self.dept_pk = 'dept_id'\n",
    "    \n",
    "    def validate_all_constraints(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Run all constraint validations and return comprehensive results\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing validation results for all checks\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'employee_pk': self._validate_primary_key(self.employees, self.emp_pk),\n",
    "            'department_pk': self._validate_primary_key(self.departments, self.dept_pk),\n",
    "            'foreign_key': self._validate_foreign_key()\n",
    "        }\n",
    "    \n",
    "    def _validate_primary_key(self, df: pd.DataFrame, pk_col: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Validate primary key constraints (uniqueness and non-null)\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            pk_col: Primary key column name\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with validation results\n",
    "        \"\"\"\n",
    "        # Check for null values\n",
    "        null_count = df[pk_col].isna().sum()\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df[df[pk_col].duplicated(keep=False)]\n",
    "        dup_count = len(duplicates)\n",
    "        \n",
    "        return {\n",
    "            'is_valid': null_count == 0 and dup_count == 0,\n",
    "            'null_count': int(null_count),\n",
    "            'duplicate_count': dup_count,\n",
    "            'duplicate_values': duplicates[pk_col].unique().tolist() if dup_count > 0 else None,\n",
    "            'offending_records': duplicates.to_dict('records') if dup_count > 0 else None\n",
    "        }\n",
    "    \n",
    "    def _validate_foreign_key(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Validate foreign key constraint (referential integrity)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with validation results\n",
    "        \"\"\"\n",
    "        # Find employees with invalid department references\n",
    "        valid_depts = set(self.departments[self.dept_pk].unique())\n",
    "        invalid_refs = self.employees[~self.employees[self.emp_fk].isin(valid_depts)]\n",
    "        invalid_count = len(invalid_refs)\n",
    "        \n",
    "        return {\n",
    "            'is_valid': invalid_count >0 \n",
    "            'invalid_count': invalid_count,\n",
    "            'invalid_values': invalid_refs[self.emp_fk].unique().tolist() if invalid_count > 0 else None,\n",
    "            'offending_records': invalid_refs[[self.emp_pk, self.emp_fk]].to_dict('records') if invalid_count > 0 else None\n",
    "        }\n",
    "    \n",
    "    def generate_report(self, results: Optional[Dict] = None) -> None:\n",
    "        \"\"\"\n",
    "        Generate a human-readable validation report\n",
    "        \n",
    "        Args:\n",
    "            results: Optional pre-computed validation results\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.validate_all_constraints()\n",
    "        \n",
    "        print(\"=== DATA CONSTRAINT VALIDATION REPORT ===\")\n",
    "        print(f\"\\nEmployee PK Validation ({self.emp_pk}):\")\n",
    "        self._print_pk_results(results['employee_pk'])\n",
    "        \n",
    "        print(f\"\\nDepartment PK Validation ({self.dept_pk}):\")\n",
    "        self._print_pk_results(results['department_pk'])\n",
    "        \n",
    "        print(f\"\\nForeign Key Validation ({self.emp_fk} → {self.dept_pk}):\")\n",
    "        self._print_fk_results(results['foreign_key'])\n",
    "    \n",
    "    def _print_pk_results(self, result: Dict) -> None:\n",
    "        \"\"\"Helper to print PK validation results\"\"\"\n",
    "        if result['is_valid']:\n",
    "            print(\"✅ Valid (no duplicates or null values)\")\n",
    "        else:\n",
    "            print(\"❌ Invalid - Found:\")\n",
    "            if result['null_count'] > :\n",
    "                print(f\" - {result['null_count']} null values\")\n",
    "            if result['duplicate_count'] > :\n",
    "                print(f\" - {result['duplicate_count']} duplicates\")\n",
    "                print(f\"   Duplicate values: {result['duplicate_values']}\")\n",
    "    \n",
    "    def _print_fk_results(self, result: Dict) -> None:\n",
    "        \"\"\"Helper to print FK validation results\"\"\"\n",
    "        if result['is_valid']:\n",
    "            print(\"✅ Valid (all references exist)\")\n",
    "        else:\n",
    "            print(f\"❌ Invalid - {result['invalid_count']} broken references\")\n",
    "            print(f\"   Invalid values: {result['invalid_values']}\")\n",
    "            print(\"\\nSample of offending records:\")\n",
    "            for i, rec in enumerate(result['offending_records'][:3], 1):\n",
    "                print(f\"   {i}. Employee {rec[self.emp_pk]} → Department {rec[self.emp_fk]}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample Data with intentional violations\n",
    "    departments = pd.DataFrame({\n",
    "        'dept_id': [10, 20, 30],\n",
    "        'dept_name': ['HR', 'Engineering', 'Finance']\n",
    "    })\n",
    "    \n",
    "    employees = pd.DataFrame({\n",
    "        'emp_id': [1, 2, 3, 4, 5, 2, None],\n",
    "        'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Bob', 'Frank'],\n",
    "        'dept_id': [10, 20, 30, 40, 20, 25, 30]\n",
    "    })\n",
    "    \n",
    "    # Initialize validator\n",
    "    validator = ConstraintValidator(employees, departments)\n",
    "    \n",
    "    # Run validations and generate report\n",
    "    validation_results = validator.validate_all_constraints()\n",
    "    validator.generate_report(validation_results)\n",
    "    \n",
    "    # Programmatic access to results\n",
    "    if not all([r['is_valid'] for r in validation_results.values()]):\n",
    "        print(\"\\nALERT: Data quality issues detected!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Drift Detection using Statistical Tests\n",
    "# Question: Implement Kolmogorov-Smirnov test using Python to detect data drift at a more sophisticated level.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def detect_ks_drift(reference_data: pd.Series, current_data: pd.Series, alpha: float = 0.05) -> dict:\n",
    "    \"\"\"\n",
    "    Detects data drift between two numerical datasets using the Kolmogorov-Smirnov test.\n",
    "\n",
    "    Args:\n",
    "        reference_data (pd.Series): The reference dataset (e.g., historical data).\n",
    "        current_data (pd.Series): The current dataset to compare against the reference.\n",
    "        alpha (float, optional): The significance level for the test. Defaults to 0.05.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the KS statistic, p-value, and a boolean indicating drift.\n",
    "    \"\"\"\n",
    "    ks_statistic, p_value = stats.ks_2samp(reference_data, current_data)\n",
    "    drift_detected = p_value < alpha\n",
    "    return {\n",
    "        \"ks_statistic\": ks_statistic,\n",
    "        \"p_value\": p_value,\n",
    "        \"drift_detected\": drift_detected,\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "# Example Usage:\n",
    "# Generate some synthetic data\n",
    "np.random.seed(42)\n",
    "reference_sample = np.random.normal(loc=0, scale=1, size=1000)\n",
    "current_sample_no_drift = np.random.normal(loc=0.1, scale=1.1, size=1000)\n",
    "current_sample_drift = np.random.normal(loc=0.5, scale=1.5, size=1000)\n",
    "\n",
    "# Convert to pandas Series for easier handling\n",
    "reference_series = pd.Series(reference_sample)\n",
    "no_drift_series = pd.Series(current_sample_no_drift)\n",
    "drift_series = pd.Series(current_sample_drift)\n",
    "\n",
    "# Detect drift when there is likely no significant drift (small changes)\n",
    "drift_report_no_drift = detect_ks_drift(reference_series, no_drift_series)\n",
    "print(\"KS Test - No Significant Drift:\")\n",
    "print(f\"  KS Statistic: {drift_report_no_drift['ks_statistic']:.4f}\")\n",
    "print(f\"  P-value: {drift_report_no_drift['p_value']:.4f}\")\n",
    "print(f\"  Drift Detected (alpha={drift_report_no_drift['alpha']}): {drift_report_no_drift['drift_detected']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Detect drift when there is a more noticeable change\n",
    "drift_report_drift = detect_ks_drift(reference_series, drift_series)\n",
    "print(\"KS Test - Significant Drift:\")\n",
    "print(f\"  KS Statistic: {drift_report_drift['ks_statistic']:.4f}\")\n",
    "print(f\"  P-value: {drift_report_drift['p_value']:.4f}\")\n",
    "print(f\"  Drift Detected (alpha={drift_report_drift['alpha']}): {drift_report_drift['drift_detected']}\")\n",
    "        \n",
    "   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
