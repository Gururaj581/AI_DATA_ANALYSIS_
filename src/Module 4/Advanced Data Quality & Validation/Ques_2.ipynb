{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrected\n",
    "# Part 1: Detecting Data Drift in AI/ML Models\n",
    "# Objective: Understand data drift, how it affects machine learning models, and techniques tomonitor it.\n",
    "# Task 1: Understanding Data Drift: Study a historical dataset used in training a simple linear regression model and\n",
    "# compare it with recent unseen data to detect drift.\n",
    "# Task 2: Monitoring Distribution Changes: Write the code to identify features that exhibit statistical distribution differences.\n",
    "# Task 3: Visualizing Data Drift: Use visualization techniques to illustrate data drift.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(100, 1) * 10\n",
    "y_train = 2.5 * X_train.squeeze() + np.random.randn(100) * 2\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "X_test_drifted = np.random.rand(100, 1) * 15  \n",
    "y_test_drifted = 2.5 * X_test_drifted.squeeze() + np.random.randn(100) * 2\n",
    "y_pred_drifted = model.predict(X_test_drifted)\n",
    "mse_drifted = mean_squared_error(y_test_drifted, y_pred_drifted)\n",
    "print(f\"Mean Squared Error on Drifted Data: {mse_drifted:.2f}\")\n",
    "from scipy.stats import ks_2samp\n",
    "ks_stat, ks_p_value = ks_2samp(X_train.squeeze(), X_test_drifted.squeeze())\n",
    "print(f\"KS Statistic: {ks_stat:.4f}, P-Value: {ks_p_value:.4f}\")\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(X_train.squeeze(), color='blue', label='Training Data', kde=True, stat='density')\n",
    "sns.histplot(X_test_drifted.squeeze(), color='red', label='Drifted Test Data', kde=True, stat='density')\n",
    "plt.title('Feature Distribution Comparison')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Automating Data Quality Checks\n",
    "# Objective: Use Python and data quality frameworks to automate validation.\n",
    "\n",
    "# Task 1: Setting Up Automated Validation with Python\n",
    "\n",
    "# Task 2: Introduction to Great Expectations: Install the great_expectations package and set up a basic project.\n",
    "\n",
    "# Task 3: Creating Expectations with Great Expectations: Use Great Expectations to define data validation expectations for a dataset.\n",
    "\n",
    "# data_validator.py\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Simple data validator for CSV files\"\"\"\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Initialize with path to CSV file\"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"file\": os.path.basename(filepath),\n",
    "            \"checks\": [],\n",
    "            \"overall_status\": \"PASSED\"\n",
    "        }\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the CSV file\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.filepath)\n",
    "            logger.info(f\"Successfully loaded {self.filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {self.filepath}: {str(e)}\")\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "            self.validation_results[\"checks\"].append({\n",
    "                \"name\": \"file_loading\",\n",
    "                \"status\": \"FAILED\",\n",
    "                \"message\": f\"Could not load file: {str(e)}\"\n",
    "            })\n",
    "            return False\n",
    "    \n",
    "    def check_missing_values(self, column, threshold=0.0):\n",
    "        \"\"\"Check if missing values in a column are below threshold\"\"\"\n",
    "        check_name = f\"missing_values_{column}\"\n",
    "        \n",
    "        if self.df is None:\n",
    "            logger.error(\"Data not loaded. Call load_data() first.\")\n",
    "            return False\n",
    "        \n",
    "        if column not in self.df.columns:\n",
    "            logger.warning(f\"Column {column} not found in dataset\")\n",
    "            self.validation_results[\"checks\"].append({\n",
    "                \"name\": check_name,\n",
    "                \"status\": \"FAILED\",\n",
    "                \"message\": f\"Column {column} not found in dataset\"\n",
    "            })\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "            return False\n",
    "        \n",
    "        missing_rate = self.df[column].isna().mean()\n",
    "        passed = missing_rate <= threshold\n",
    "        \n",
    "        self.validation_results[\"checks\"].append({\n",
    "            \"name\": check_name,\n",
    "            \"status\": \"PASSED\" if passed else \"FAILED\",\n",
    "            \"message\": f\"Missing rate: {missing_rate:.2%} (threshold: {threshold:.2%})\",\n",
    "            \"details\": {\n",
    "                \"missing_count\": int(self.df[column].isna().sum()),\n",
    "                \"total_count\": len(self.df),\n",
    "                \"missing_rate\": float(missing_rate)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        if not passed:\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "        \n",
    "        logger.info(f\"Check {check_name}: {'PASSED' if passed else 'FAILED'}\")\n",
    "        return passed\n",
    "    \n",
    "    def check_value_range(self, column, min_val=None, max_val=None):\n",
    "        \"\"\"Check if values in a column are within specified range\"\"\"\n",
    "        check_name = f\"value_range_{column}\"\n",
    "        \n",
    "        if self.df is None:\n",
    "            logger.error(\"Data not loaded. Call load_data() first.\")\n",
    "            return False\n",
    "        \n",
    "        if column not in self.df.columns:\n",
    "            logger.warning(f\"Column {column} not found in dataset\")\n",
    "            self.validation_results[\"checks\"].append({\n",
    "                \"name\": check_name,\n",
    "                \"status\": \"FAILED\",\n",
    "                \"message\": f\"Column {column} not found in dataset\"\n",
    "            })\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "            return False\n",
    "        \n",
    "        # Get non-null values\n",
    "        values = self.df[column].dropna()\n",
    "        \n",
    "        # Check minimum value\n",
    "        min_check_passed = True\n",
    "        if min_val is not None:\n",
    "            min_check_passed = values.min() >= min_val\n",
    "        \n",
    "        # Check maximum value\n",
    "        max_check_passed = True\n",
    "        if max_val is not None:\n",
    "            max_check_passed = values.max() <= max_val\n",
    "        \n",
    "        passed = min_check_passed and max_check_passed\n",
    "        \n",
    "        self.validation_results[\"checks\"].append({\n",
    "            \"name\": check_name,\n",
    "            \"status\": \"PASSED\" if passed else \"FAILED\",\n",
    "            \"message\": f\"Range check: {'PASSED' if passed else 'FAILED'}\",\n",
    "            \"details\": {\n",
    "                \"min_value\": float(values.min()),\n",
    "                \"max_value\": float(values.max()),\n",
    "                \"min_threshold\": min_val,\n",
    "                \"max_threshold\": max_val\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        if not passed:\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "        \n",
    "        logger.info(f\"Check {check_name}: {'PASSED' if passed else 'FAILED'}\")\n",
    "        return passed\n",
    "    \n",
    "    def check_unique_values(self, column, expected_unique_count=None, min_unique_rate=None):\n",
    "        \"\"\"Check uniqueness of values in a column\"\"\"\n",
    "        check_name = f\"unique_values_{column}\"\n",
    "        \n",
    "        if self.df is None:\n",
    "            logger.error(\"Data not loaded. Call load_data() first.\")\n",
    "            return False\n",
    "        \n",
    "        if column not in self.df.columns:\n",
    "            logger.warning(f\"Column {column} not found in dataset\")\n",
    "            self.validation_results[\"checks\"].append({\n",
    "                \"name\": check_name,\n",
    "                \"status\": \"FAILED\",\n",
    "                \"message\": f\"Column {column} not found in dataset\"\n",
    "            })\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "            return False\n",
    "        \n",
    "        # Get non-null values\n",
    "        values = self.df[column].dropna()\n",
    "        unique_count = values.nunique()\n",
    "        unique_rate = unique_count / len(values) if len(values) > 0 else 0\n",
    "        \n",
    "        # Check if uniqueness meets criteria\n",
    "        count_check_passed = True\n",
    "        if expected_unique_count is not None:\n",
    "            count_check_passed = unique_count == expected_unique_count\n",
    "        \n",
    "        rate_check_passed = True\n",
    "        if min_unique_rate is not None:\n",
    "            rate_check_passed = unique_rate >= min_unique_rate\n",
    "        \n",
    "        passed = count_check_passed and rate_check_passed\n",
    "        \n",
    "        self.validation_results[\"checks\"].append({\n",
    "            \"name\": check_name,\n",
    "            \"status\": \"PASSED\" if passed else \"FAILED\",\n",
    "            \"message\": f\"Uniqueness check: {'PASSED' if passed else 'FAILED'}\",\n",
    "            \"details\": {\n",
    "                \"unique_count\": int(unique_count),\n",
    "                \"total_count\": int(len(values)),\n",
    "                \"unique_rate\": float(unique_rate),\n",
    "                \"expected_unique_count\": expected_unique_count,\n",
    "                \"min_unique_rate\": min_unique_rate\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        if not passed:\n",
    "            self.validation_results[\"overall_status\"] = \"FAILED\"\n",
    "        \n",
    "        logger.info(f\"Check {check_name}: {'PASSED' if passed else 'FAILED'}\")\n",
    "        return passed\n",
    "    \n",
    "    def save_results(self, output_file=\"validation_results.json\"):\n",
    "        \"\"\"Save validation results to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(self.validation_results, f, indent=2)\n",
    "            logger.info(f\"Validation results saved to {output_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save validation results: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def run_all_checks(self, checks_config):\n",
    "        \"\"\"Run all checks specified in config\"\"\"\n",
    "        if not self.load_data():\n",
    "            return False\n",
    "        \n",
    "        for check in checks_config:\n",
    "            check_type = check[\"type\"]\n",
    "            \n",
    "            if check_type == \"missing_values\":\n",
    "                self.check_missing_values(\n",
    "                    column=check[\"column\"],\n",
    "                    threshold=check.get(\"threshold\", 0.0)\n",
    "                )\n",
    "            elif check_type == \"value_range\":\n",
    "                self.check_value_range(\n",
    "                    column=check[\"column\"],\n",
    "                    min_val=check.get(\"min_val\"),\n",
    "                    max_val=check.get(\"max_val\")\n",
    "                )\n",
    "            elif check_type == \"unique_values\":\n",
    "                self.check_unique_values(\n",
    "                    column=check[\"column\"],\n",
    "                    expected_unique_count=check.get(\"expected_unique_count\"),\n",
    "                    min_unique_rate=check.get(\"min_unique_rate\")\n",
    "                )\n",
    "        \n",
    "        logger.info(f\"All checks completed. Overall status: {self.validation_results['overall_status']}\")\n",
    "        return self.validation_results[\"overall_status\"] == \"PASSED\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
