{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI & Machine Learning for Data Quality\n",
    "**Description**: AI and machine learning can automate and enhance data quality checks by learning patterns and identifying anomalies more effectively than static rules.\n",
    "\n",
    "**Task 1**: Training a model to predict and flag unusual trend patterns in sales data that\n",
    "deviate from historical norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sales_data\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Example Usage for Task 1:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Generate some synthetic sales data with an unusual spike\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     63\u001b[0m dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     64\u001b[0m normal_sales \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;28mlen\u001b[39m(dates))) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dates))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_anomaly_detection_model(sales_data: pd.DataFrame, time_column: str, sales_column: str, contamination: float = 0.05):\n",
    "    \"\"\"\n",
    "    Trains an Isolation Forest model to detect unusual trend patterns in sales data.\n",
    "\n",
    "    Args:\n",
    "        sales_data (pd.DataFrame): DataFrame with sales data, including a time column and a sales column.\n",
    "        time_column (str): Name of the time column.\n",
    "        sales_column (str): Name of the sales column.\n",
    "        contamination (float, optional): The proportion of outliers expected in the dataset. Defaults to 0.05.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained Isolation Forest model and the scaled sales data.\n",
    "    \"\"\"\n",
    "    if time_column not in sales_data.columns or sales_column not in sales_data.columns:\n",
    "        raise ValueError(\"Time column or sales column not found in DataFrame.\")\n",
    "\n",
    "    # Sort data by time\n",
    "    sales_data = sales_data.sort_values(by=time_column).reset_index(drop=True)\n",
    "\n",
    "    # Scale the sales data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_sales = scaler.fit_transform(sales_data[[sales_column]])\n",
    "\n",
    "    # Train Isolation Forest model\n",
    "    model = IsolationForest(contamination=contamination, random_state=42)\n",
    "    model.fit(scaled_sales)\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "def predict_unusual_trends(sales_data: pd.DataFrame, model: IsolationForest, scaler: StandardScaler, sales_column: str):\n",
    "    \"\"\"\n",
    "    Predicts unusual trend patterns in new sales data using the trained Isolation Forest model.\n",
    "\n",
    "    Args:\n",
    "        sales_data (pd.DataFrame): DataFrame with new sales data.\n",
    "        model (IsolationForest): Trained Isolation Forest model.\n",
    "        scaler (StandardScaler): Fitted StandardScaler object.\n",
    "        sales_column (str): Name of the sales column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'is_unusual_trend' indicating\n",
    "                      whether a data point is predicted as an unusual trend (True) or not (False).\n",
    "    \"\"\"\n",
    "    if sales_column not in sales_data.columns:\n",
    "        raise ValueError(\"Sales column not found in DataFrame.\")\n",
    "\n",
    "    scaled_sales = scaler.transform(sales_data[[sales_column]])\n",
    "    outlier_predictions = model.predict(scaled_sales)\n",
    "    # Isolation Forest returns -1 for outliers and 1 for inliers\n",
    "    sales_data['is_unusual_trend'] = np.where(outlier_predictions == -1, True, False)\n",
    "    return sales_data\n",
    "\n",
    "# Example Usage for Task 1:\n",
    "# Generate some synthetic sales data with an unusual spike\n",
    "np.random.seed(42)\n",
    "dates = pd.to_datetime(pd.date_range(start='2024-01-01', end='2024-12-31', freq='D'))\n",
    "normal_sales = 100 + 10 * np.sin(np.linspace(0, 10 * np.pi, len(dates))) + np.random.normal(0, 5, len(dates))\n",
    "unusual_spike_index = len(dates) // 2\n",
    "normal_sales[unusual_spike_index:unusual_spike_index+7] += 50  # Simulate a week-long spike\n",
    "sales_df = pd.DataFrame({'date': dates, 'sales': normal_sales})\n",
    "\n",
    "# Train the anomaly detection model\n",
    "anomaly_model, sales_scaler = train_anomaly_detection_model(sales_df.copy(), time_column='date', sales_column='sales', contamination=0.02)\n",
    "\n",
    "# Predict unusual trends on the same data\n",
    "sales_df_with_predictions = predict_unusual_trends(sales_df.copy(), anomaly_model, sales_scaler, sales_column='sales')\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='date', y='sales', data=sales_df_with_predictions, label='Sales')\n",
    "unusual_trends = sales_df_with_predictions[sales_df_with_predictions['is_unusual_trend']]\n",
    "sns.scatterplot(x='date', y='sales', data=unusual_trends, color='red', label='Unusual Trend')\n",
    "plt.title('Sales Data with Unusual Trend Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask 1: Unusual Trend Detection Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Using clustering algorithms to detect duplicate records where entries are not\n",
    "exactly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# Task 2 was implemented in the previous response. Please refer to that code block.\n",
    "\n",
    "print(\"\\nTask 2: Near Duplicate Detection using Clustering (Implementation in previous response)\")\n",
    "\n",
    "# Task 3: Implementing classification models to validate data based on learned characteristics from labeled datasets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_data_validation_model(labeled_data: pd.DataFrame, feature_columns: list, target_column: str):\n",
    "    \"\"\"\n",
    "    Trains a classification model to validate data based on learned characteristics.\n",
    "\n",
    "    Args:\n",
    "        labeled_data (pd.DataFrame): DataFrame with labeled data (valid/invalid).\n",
    "        feature_columns (list): List of column names to use as features.\n",
    "        target_column (str): Name of the column indicating data validity (e.g., 'is_valid').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained classification model and the feature columns.\n",
    "    \"\"\"\n",
    "    if not feature_columns or target_column not in labeled_data.columns or not all(col in labeled_data.columns for col in feature_columns):\n",
    "        raise ValueError(\"Feature columns or target column not found in DataFrame.\")\n",
    "\n",
    "    X = labeled_data[feature_columns]\n",
    "    y = labeled_data[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Data Validation Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model, feature_columns\n",
    "\n",
    "def validate_new_data(new_data: pd.DataFrame, model: RandomForestClassifier, feature_columns: list):\n",
    "    \"\"\"\n",
    "    Validates new data using the trained classification model.\n",
    "\n",
    "    Args:\n",
    "        new_data (pd.DataFrame): DataFrame with new data to validate.\n",
    "        model (RandomForestClassifier): Trained classification model.\n",
    "        feature_columns (list): List of feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'predicted_validity'\n",
    "                      indicating the model's prediction of data validity (True/False).\n",
    "    \"\"\"\n",
    "    if not all(col in new_data.columns for col in feature_columns):\n",
    "        raise ValueError(\"Not all feature columns found in the new data.\")\n",
    "\n",
    "    X_new = new_data[feature_columns]\n",
    "    predictions = model.predict(X_new)\n",
    "    new_data['predicted_validity'] = predictions.astype(bool)\n",
    "    return new_data\n",
    "\n",
    "# Example Usage for Task 3:\n",
    "# Create synthetic labeled data\n",
    "np.random.seed(42)\n",
    "data_labeled = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'error_code': np.random.choice(['A10', 'B25', 'C30', None], size=100),\n",
    "    'value': np.random.randint(1, 100, size=100),\n",
    "    'is_valid': [True] * 80 + [False] * 20  # Simulate some invalid records\n",
    "}\n",
    "df_labeled = pd.DataFrame(data_labeled)\n",
    "\n",
    "# Define features and target\n",
    "features = ['feature1', 'feature2', 'value']\n",
    "target = 'is_valid'\n",
    "\n",
    "# Train the data validation model\n",
    "validation_model, model_features = train_data_validation_model(df_labeled.copy(), feature_columns=features, target_column=target)\n",
    "\n",
    "# Create new data to validate\n",
    "data_new = {\n",
    "    'feature1': np.random.rand(20),\n",
    "    'feature2': np.random.rand(20),\n",
    "    'error_code': np.random.choice(['A10', 'B25', None, 'D40'], size=20),\n",
    "    'value': np.random.randint(1, 100, size=20)\n",
    "}\n",
    "df_new = pd.DataFrame(data_new)\n",
    "\n",
    "# Validate the new data\n",
    "df_new_validated = validate_new_data(df_new.copy(), validation_model, feature_columns=model_features)\n",
    "print(\"\\nTask 3: Data Validation using Classification Model\")\n",
    "print(df_new_validated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implementing classification models to validate data based on learned\n",
    "characteristics from labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "\n",
    "# Task 3: Implementing classification models to validate data based on learned characteristics from labeled datasets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_data_validation_model(labeled_data: pd.DataFrame, feature_columns: list, target_column: str):\n",
    "    \"\"\"\n",
    "    Trains a classification model to validate data based on learned characteristics.\n",
    "\n",
    "    Args:\n",
    "        labeled_data (pd.DataFrame): DataFrame with labeled data (valid/invalid).\n",
    "        feature_columns (list): List of column names to use as features.\n",
    "        target_column (str): Name of the column indicating data validity (e.g., 'is_valid').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained classification model and the feature columns.\n",
    "    \"\"\"\n",
    "    if not feature_columns or target_column not in labeled_data.columns or not all(col in labeled_data.columns for col in feature_columns):\n",
    "        raise ValueError(\"Feature columns or target column not found in DataFrame.\")\n",
    "\n",
    "    X = labeled_data[feature_columns]\n",
    "    y = labeled_data[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Data Validation Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model, feature_columns\n",
    "\n",
    "def validate_new_data(new_data: pd.DataFrame, model: RandomForestClassifier, feature_columns: list):\n",
    "    \"\"\"\n",
    "    Validates new data using the trained classification model.\n",
    "\n",
    "    Args:\n",
    "        new_data (pd.DataFrame): DataFrame with new data to validate.\n",
    "        model (RandomForestClassifier): Trained classification model.\n",
    "        feature_columns (list): List of feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'predicted_validity'\n",
    "                      indicating the model's prediction of data validity (True/False).\n",
    "    \"\"\"\n",
    "    if not all(col in new_data.columns for col in feature_columns):\n",
    "        raise ValueError(\"Not all feature columns found in the new data.\")\n",
    "\n",
    "    X_new = new_data[feature_columns]\n",
    "    predictions = model.predict(X_new)\n",
    "    new_data['predicted_validity'] = predictions.astype(bool)\n",
    "    return new_data\n",
    "\n",
    "# Example Usage for Task 3:\n",
    "# Create synthetic labeled data\n",
    "np.random.seed(42)\n",
    "data_labeled = {\n",
    "    'feature1': np.random.rand(100),\n",
    "    'feature2': np.random.rand(100),\n",
    "    'error_code': np.random.choice(['A10', 'B25', 'C30', None], size=100),\n",
    "    'value': np.random.randint(1, 100, size=100),\n",
    "    'is_valid': [True] * 80 + [False] * 20  # Simulate some invalid records\n",
    "}\n",
    "df_labeled = pd.DataFrame(data_labeled)\n",
    "\n",
    "# Define features and target\n",
    "features = ['feature1', 'feature2', 'value']\n",
    "target = 'is_valid'\n",
    "\n",
    "# Train the data validation model\n",
    "validation_model, model_features = train_data_validation_model(df_labeled.copy(), feature_columns=features, target_column=target)\n",
    "\n",
    "# Create new data to validate\n",
    "data_new = {\n",
    "    'feature1': np.random.rand(20),\n",
    "    'feature2': np.random.rand(20),\n",
    "    'error_code': np.random.choice(['A10', 'B25', None, 'D40'], size=20),\n",
    "    'value': np.random.randint(1, 100, size=20)\n",
    "}\n",
    "df_new = pd.DataFrame(data_new)\n",
    "\n",
    "# Validate the new data\n",
    "df_new_validated = validate_new_data(df_new.copy(), validation_model, feature_columns=model_features)\n",
    "print(\"\\nTask 3: Data Validation using Classification Model\")\n",
    "print(df_new_validated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
