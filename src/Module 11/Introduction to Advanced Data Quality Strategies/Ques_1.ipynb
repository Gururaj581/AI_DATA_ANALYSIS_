{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Profiling to Understand Data Quality\n",
    "**Description**: Use basic statistical methods to profile a dataset and identify potential quality issues.\n",
    "\n",
    "**Steps**:\n",
    "1. Load the dataset using pandas in Python.\n",
    "2. Understand the data by checking its basic statistics.\n",
    "3. Identify null values.\n",
    "4. Check unique values for categorical columns.\n",
    "5. Review outliers using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here corrected \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the dataset using pandas in Python.\n",
    "# Replace this with your actual data loading\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male'],\n",
    "    'Age': [25, 30, np.nan, 40, 22, 35, 28, 32, 45, 27],\n",
    "    'Income': [50000, 60000, 75000, 80000, 55000, np.nan, 65000, 70000, 90000, 52000],\n",
    "    'City': ['Bangalore', 'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Mumbai', 'Delhi', 'Bangalore', 'Mumbai', 'Chennai'],\n",
    "    'Score': [85, 78, 92, 68, 80, 75, 88, 95, 70, 82],\n",
    "    'Category': ['A', 'B', 'A', 'C', 'B', 'A', 'B', 'C', 'A', 'B']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Loaded DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Step 2: Understand the data by checking its basic statistics.\n",
    "print(\"Basic Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Step 3: Identify null values.\n",
    "print(\"Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Step 4: Check unique values for categorical columns.\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "print(\"Unique Values in Categorical Columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"Column '{col}': {df[col].nunique()} unique values - {df[col].unique()}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Step 5: Review outliers using box plots.\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "print(\"Box Plots for Numerical Columns:\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(2, len(numerical_cols) // 2 + (len(numerical_cols) % 2), i + 1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement Simple Data Validation\n",
    "**Description**: Write a Python script to validate the data types and constraints of each column in a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Define constraints for each column.\n",
    "2. Validate each column based on its constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here corrected \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_data(df: pd.DataFrame, constraints: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates the data types and constraints of each column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to validate.\n",
    "        constraints (dict): A dictionary defining the constraints for each column.\n",
    "                           The keys are column names, and the values are dictionaries\n",
    "                           specifying the expected 'dtype' and other constraints\n",
    "                           (e.g., 'min', 'max', 'allowed_values').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the validation results for each column.\n",
    "              Keys are column names, and values are lists of validation errors\n",
    "              found in that column.\n",
    "    \"\"\"\n",
    "    validation_errors = {}\n",
    "    for col, rules in constraints.items():\n",
    "        validation_errors[col] = []\n",
    "        if col not in df.columns:\n",
    "            validation_errors[col].append(f\"Column '{col}' not found in DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        series = df[col]\n",
    "\n",
    "        # Validate data type\n",
    "        expected_dtype = rules.get('dtype')\n",
    "        if expected_dtype:\n",
    "            try:\n",
    "                if expected_dtype == 'int':\n",
    "                    if not pd.api.types.is_integer_dtype(series):\n",
    "                        validation_errors[col].append(f\"Column '{col}' should be of integer type, but is {series.dtype}.\")\n",
    "                elif expected_dtype == 'float':\n",
    "                    if not pd.api.types.is_float_dtype(series):\n",
    "                        validation_errors[col].append(f\"Column '{col}' should be of float type, but is {series.dtype}.\")\n",
    "                elif expected_dtype == 'datetime':\n",
    "                    try:\n",
    "                        pd.to_datetime(series, errors='raise')\n",
    "                    except Exception:\n",
    "                        validation_errors[col].append(f\"Column '{col}' should be of datetime type, but contains invalid formats.\")\n",
    "                elif expected_dtype == 'category':\n",
    "                    pass # pandas 'category' dtype can handle various underlying types\n",
    "                elif expected_dtype == 'object':\n",
    "                    pass # 'object' is a general type, further constraints might be needed\n",
    "                elif expected_dtype != str(series.dtype):\n",
    "                    validation_errors[col].append(f\"Column '{col}' should be of type '{expected_dtype}', but is '{series.dtype}'.\")\n",
    "            except AttributeError:\n",
    "                validation_errors[col].append(f\"Invalid dtype specified for column '{col}': {expected_dtype}\")\n",
    "\n",
    "        # Validate minimum value\n",
    "        min_val = rules.get('min')\n",
    "        if min_val is not None and pd.api.types.is_numeric_dtype(series):\n",
    "            invalid_min = series[series < min_val]\n",
    "            if not invalid_min.empty:\n",
    "                validation_errors[col].append(f\"Values in column '{col}' are below the minimum allowed value ({min_val}): {invalid_min.tolist()}\")\n",
    "\n",
    "        # Validate maximum value\n",
    "        max_val = rules.get('max')\n",
    "        if max_val is not None and pd.api.types.is_numeric_dtype(series):\n",
    "            invalid_max = series[series > max_val]\n",
    "            if not invalid_max.empty:\n",
    "                validation_errors[col].append(f\"Values in column '{col}' exceed the maximum allowed value ({max_val}): {invalid_max.tolist()}\")\n",
    "\n",
    "        # Validate allowed values (for categorical columns)\n",
    "        allowed_values = rules.get('allowed_values')\n",
    "        if allowed_values and pd.api.types.is_object_dtype(series):\n",
    "            invalid_values = series[~series.isin(allowed_values)]\n",
    "            if not invalid_values.empty:\n",
    "                validation_errors[col].append(f\"Values in column '{col}' are not in the allowed set ({allowed_values}): {invalid_values.unique().tolist()}\")\n",
    "\n",
    "        # Add more validation rules as needed (e.g., regex patterns, unique values)\n",
    "\n",
    "        if not validation_errors[col]:\n",
    "            validation_errors[col].append(\"Column passed validation.\")\n",
    "\n",
    "    return validation_errors\n",
    "\n",
    "# Step 1: Define constraints for each column.\n",
    "data_constraints = {\n",
    "    'ID': {'dtype': 'int', 'min': 1},\n",
    "    'Gender': {'dtype': 'category', 'allowed_values': ['Male', 'Female']},\n",
    "    'Age': {'dtype': 'int', 'min': 18, 'max': 100},\n",
    "    'Income': {'dtype': 'float', 'min': 0},\n",
    "    'City': {'dtype': 'object'},\n",
    "    'Score': {'dtype': 'int', 'min': 0, 'max': 100},\n",
    "    'Category': {'dtype': 'category', 'allowed_values': ['A', 'B', 'C']}\n",
    "}\n",
    "\n",
    "# Step 2: Validate each column based on its constraints.\n",
    "# Load a sample DataFrame (replace with your actual loading)\n",
    "data = {\n",
    "    'ID': [1, 2, 0, 4, 5],\n",
    "    'Gender': ['Male', 'Female', 'Other', 'Female', 'Male'],\n",
    "    'Age': [25, 15, 30, 110, 22],\n",
    "    'Income': [50000.0, -100, 75000.0, 80000.0, np.nan],\n",
    "    'City': ['Bangalore', 'Mumbai', 'Delhi', 'Bangalore', 'Chennai'],\n",
    "    'Score': [85, 78, 105, 68, 80],\n",
    "    'Category': ['A', 'B', 'D', 'C', 'B']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "validation_results = validate_data(df, data_constraints)\n",
    "\n",
    "print(\"Data Validation Results:\")\n",
    "for column, errors in validation_results.items():\n",
    "    print(f\"\\nColumn '{column}':\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Detect Missing Data Patterns\n",
    "**Description**: Analyze and visualize missing data patterns in a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Visualize missing data using a heatmap.\n",
    "2. Identify patterns in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here corrected \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def visualize_missing_heatmap(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualizes missing data in a DataFrame using a heatmap.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Data Heatmap')\n",
    "    plt.show()\n",
    "def identify_missing_patterns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Identifies potential patterns in missing data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "    total_rows = len(df)\n",
    "    missing_percentage = (null_counts / total_rows) * 100\n",
    "\n",
    "    print(\"Missing Data Summary:\")\n",
    "    missing_info = pd.DataFrame({'Missing Count': null_counts, 'Missing Percentage': missing_percentage})\n",
    "    print(missing_info[missing_info['Missing Count'] > 0])\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "    # Investigate co-occurrence of missing values\n",
    "    missing_matrix = df.isnull()\n",
    "    missing_combinations = {}\n",
    "    for i, row in missing_matrix.iterrows():\n",
    "        missing_cols = tuple(row[row].index.sort_values().tolist())\n",
    "        if missing_cols:\n",
    "            missing_combinations[missing_cols] = missing_combinations.get(missing_cols, 0) + 1\n",
    "\n",
    "    if missing_combinations:\n",
    "        print(\"Co-occurrence of Missing Values:\")\n",
    "        for cols, count in sorted(missing_combinations.items(), key=lambda item: item[1], reverse=True):\n",
    "            print(f\"Columns '{cols}': {count} missing rows\")\n",
    "        print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No co-occurrence of missing values found.\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "    # Further analysis: Check if missingness in one column affects another\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                missing_in_col1 = df[df[col1].isnull()]\n",
    "                percentage_missing_in_col2_when_col1_missing = (missing_in_col1[col2].isnull().sum() / len(missing_in_col1)) * 100 if len(missing_in_col1) > 0 else 0\n",
    "                if percentage_missing_in_col2_when_col1_missing > 0:\n",
    "                    print(f\"Percentage of missing values in '{col2}' when '{col1}' is missing: {percentage_missing_in_col2_when_col1_missing:.2f}%\")\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Sample DataFrame with missing data\n",
    "data_missing = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Age': [25, np.nan, 30, np.nan, 22, 35, 28, 32, 45, 27],\n",
    "    'Income': [50000, 60000, np.nan, 80000, 55000, np.nan, 65000, 70000, 90000, 52000],\n",
    "    'Education': ['Bachelor', 'Master', 'PhD', 'Bachelor', np.nan, 'Master', 'Bachelor', 'PhD', np.nan, 'Bachelor'],\n",
    "    'Occupation': ['Engineer', np.nan, 'Scientist', 'Analyst', 'Engineer', np.nan, 'Scientist', 'Analyst', 'Manager', 'Engineer'],\n",
    "    'City': ['Bangalore', 'Mumbai', 'Delhi', 'Bangalore', 'Chennai', 'Mumbai', 'Delhi', np.nan, 'Mumbai', 'Chennai'],\n",
    "    'Score': [85, 78, 92, 68, 80, 75, np.nan, 95, 70, 82]\n",
    "}\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "\n",
    "# Step 1: Visualize missing data\n",
    "visualize_missing_heatmap(df_missing.copy())\n",
    "\n",
    "# Step 2: Identify patterns in missing data\n",
    "identify_missing_patterns(df_missing.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Integrate Automated Data Quality Checks\n",
    "**Description**: Integrate automated data quality checks using the Great Expectations library for a dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Install and initialize Great Expectations.\n",
    "2. Set up Great Expectations.\n",
    "3. Add further checks and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
