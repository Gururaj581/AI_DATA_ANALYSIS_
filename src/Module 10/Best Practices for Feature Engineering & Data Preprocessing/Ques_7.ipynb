{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Training Data:\n",
      "     MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  2.475751  0.266201  1.500473  -0.266636   -0.900449 -0.219674  1.441017   \n",
      "1  2.462605 -1.251479  0.863503  -0.751121    1.186500 -0.830714  1.235934   \n",
      "2  1.885951  1.100925  2.613913   0.196463   -0.725783  0.118538  1.133393   \n",
      "3  0.994291  1.100925  0.504211   0.192853   -0.663546 -0.230108  1.133393   \n",
      "4  0.001771  1.100925  0.900830   0.267694   -0.656520 -0.732521  1.133393   \n",
      "\n",
      "   Longitude  \n",
      "0  -0.640477  \n",
      "1  -0.554449  \n",
      "2  -0.726504  \n",
      "3  -0.812532  \n",
      "4  -0.812532  \n",
      "\n",
      "Training Data Imputer: SimpleImputer()\n",
      "Training Data Scaler: StandardScaler()\n",
      "\n",
      "Processed Inference Data:\n",
      "        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "1000 -0.253470 -0.644407 -0.328576  -0.462455    0.702657 -0.012564 -0.507273   \n",
      "1001 -0.160068 -0.113219 -0.590783  -0.488773   -0.270048 -0.222131 -0.609815   \n",
      "1002 -0.265788 -0.264987 -0.095526   0.260036   -0.062256 -0.282455 -0.507273   \n",
      "1003  0.281868 -0.189103 -0.204142  -0.227801    0.544053  0.497477 -0.507273   \n",
      "1004  0.382617 -0.947943  0.671585  -0.176890    1.988555  0.466879 -0.507273   \n",
      "\n",
      "      Longitude  \n",
      "1000   3.402828  \n",
      "1001   3.316800  \n",
      "1002   3.230773  \n",
      "1003   3.230773  \n",
      "1004   3.144745  \n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "def preprocess_data(df, is_training=True, imputer=None, scaler=None):\n",
    "    df_processed = df.copy()\n",
    "    numerical_cols = df_processed.select_dtypes(include=np.number).columns\n",
    "    if is_training:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df_processed[numerical_cols] = imputer.fit_transform(df_processed[numerical_cols])\n",
    "    elif imputer is not None:\n",
    "        df_processed[numerical_cols] = imputer.transform(df_processed[numerical_cols])\n",
    "    else:\n",
    "        raise ValueError(\"Imputer must be provided for inference.\")\n",
    "    if is_training:\n",
    "        scaler = StandardScaler()\n",
    "        df_processed[numerical_cols] = scaler.fit_transform(df_processed[numerical_cols])\n",
    "    elif scaler is not None:\n",
    "        df_processed[numerical_cols] = scaler.transform(df_processed[numerical_cols])\n",
    "    else:\n",
    "        raise ValueError(\"Scaler must be provided for inference.\")\n",
    "\n",
    "    return df_processed, imputer, scaler\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "df_train = df_train.iloc[:1000] \n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "\n",
    "# Preprocess training data\n",
    "X_train_processed, imputer, scaler = preprocess_data(X_train)\n",
    "print(\"Processed Training Data:\")\n",
    "print(X_train_processed.head())\n",
    "print(\"\\nTraining Data Imputer:\", imputer)\n",
    "print(\"Training Data Scaler:\", scaler)\n",
    "\n",
    "# Simulate inference data\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "df_inference = df_inference.iloc[1000:1100].copy() # Sample a different subset\n",
    "# Introduce some missing values in inference data\n",
    "df_inference.iloc[[5, 15], 0] = np.nan\n",
    "df_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore')\n",
    "\n",
    "# Preprocess inference data using the fitted imputer and scaler\n",
    "X_inference_processed, _, _ = preprocess_data(df_inference, is_training=False, imputer=imputer, scaler=scaler)\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(X_inference_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Inference Data:\n",
      "[[ 2.33397811  0.98330419  0.64880421 -0.1682248  -0.97135592 -0.04676\n",
      "   1.04692041 -1.32016306]\n",
      " [ 2.32147966 -0.60373066  0.33632187 -0.28809752  0.85710482 -0.0852553\n",
      "   1.03755975 -1.31517701]\n",
      " [ 1.77322836  1.85617335  1.19503099 -0.05364345 -0.81832458 -0.02545269\n",
      "   1.03287942 -1.32514912]\n",
      " [ 0.92548692  1.85617335  0.16006206 -0.05453673 -0.76379617 -0.04741729\n",
      "   1.03287942 -1.33013518]\n",
      " [-0.01814599  1.85617335  0.35463383 -0.03601943 -0.75763973 -0.07906917\n",
      "   1.03287942 -1.33013518]]\n",
      "\n",
      "Mean Squared Error on Training Data: 0.52\n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data for training\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "numerical_features = X_train_sample.select_dtypes(include=np.number).columns\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the full training pipeline\n",
    "train_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "train_pipeline.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Simulate inference data\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "X_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore').iloc[:50]\n",
    "# Introduce some missing values in inference data\n",
    "X_inference.iloc[[5, 15], 0] = np.nan\n",
    "\n",
    "# Create an inference pipeline using the *same* preprocessor\n",
    "inference_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', train_pipeline.named_steps['preprocessor'])\n",
    "])\n",
    "\n",
    "# Process the inference data\n",
    "X_inference_processed = inference_pipeline.transform(X_inference)\n",
    "print(\"Processed Inference Data:\")\n",
    "print(X_inference_processed[:5])\n",
    "\n",
    "# Evaluate the training pipeline (optional, for demonstration)\n",
    "y_pred_train = train_pipeline.predict(X_train_sample)\n",
    "mse_train = mean_squared_error(y_train_sample, y_pred_train)\n",
    "print(f\"\\nMean Squared Error on Training Data: {mse_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing models (imputer.joblib, scaler.joblib) saved successfully.\n",
      "\n",
      "Preprocessing models loaded successfully.\n",
      "\n",
      "Processed Inference Data:\n",
      "     MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  2.333978  0.983304  0.648804  -0.168225   -0.971356 -0.046760  1.046920   \n",
      "1  2.321480 -0.603731  0.336322  -0.288098    0.857105 -0.085255  1.037560   \n",
      "2  1.773228  1.856173  1.195031  -0.053643   -0.818325 -0.025453  1.032879   \n",
      "3  0.925487  1.856173  0.160062  -0.054537   -0.763796 -0.047417  1.032879   \n",
      "4 -0.018146  1.856173  0.354634  -0.036019   -0.757640 -0.079069  1.032879   \n",
      "\n",
      "   Longitude  \n",
      "0  -1.320163  \n",
      "1  -1.315177  \n",
      "2  -1.325149  \n",
      "3  -1.330135  \n",
      "4  -1.330135  \n"
     ]
    }
   ],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load data for training\n",
    "housing_train = fetch_california_housing(as_frame=True)\n",
    "df_train = housing_train.frame.copy()\n",
    "X_train = df_train.drop('MedHouseVal', axis=1)\n",
    "y_train = df_train['MedHouseVal']\n",
    "X_train_sample, _, _, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the preprocessing steps\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train_sample)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_sample)\n",
    "\n",
    "# Save the fitted preprocessors\n",
    "joblib.dump(imputer, 'imputer.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"Preprocessing models (imputer.joblib, scaler.joblib) saved successfully.\")\n",
    "\n",
    "# Simulate inference data\n",
    "housing_inference = fetch_california_housing(as_frame=True)\n",
    "df_inference = housing_inference.frame.copy()\n",
    "X_inference = df_inference.drop('MedHouseVal', axis=1, errors='ignore').iloc[:50]\n",
    "# Introduce some missing values in inference data\n",
    "X_inference.iloc[[5, 15], 0] = np.nan\n",
    "\n",
    "# Load the saved preprocessors\n",
    "loaded_imputer = joblib.load('imputer.joblib')\n",
    "loaded_scaler = joblib.load('scaler.joblib')\n",
    "print(\"\\nPreprocessing models loaded successfully.\")\n",
    "\n",
    "# Apply the loaded preprocessors to the inference data\n",
    "numerical_cols_inference = X_inference.select_dtypes(include=np.number).columns\n",
    "X_inference_imputed = pd.DataFrame(loaded_imputer.transform(X_inference[numerical_cols_inference]),\n",
    "                                   columns=numerical_cols_inference, index=X_inference.index)\n",
    "X_inference_scaled = pd.DataFrame(loaded_scaler.transform(X_inference_imputed),\n",
    "                                  columns=numerical_cols_inference, index=X_inference_imputed.index)\n",
    "\n",
    "print(\"\\nProcessed Inference Data:\")\n",
    "print(X_inference_scaled.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
